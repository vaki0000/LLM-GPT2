# LLM-GPT2
**Description**: In this tutorial series, we will learn and use the ðŸ¤— Hugging Face Transformer API

* how to build and preprocess a Custom Dataset from a CSV file with the ðŸ¤— Hugging Face Datasets API
* how to train a ðŸ¤— Hugging Face Tokenizer from scratch with the ðŸ¤— Hugging Face Tokenizer API
* how to train a Causal Language Transformer Model from scratch
* how to push (upload) a Model, Dataset, and Tokenizer to the ðŸ¤— Hugging Face Hub
* how to download and use a Model, Dataset, and Tokenizer from the ðŸ¤— Hugging Face Hub
* how to generate text using the ðŸ¤— Hugging Face Text Generation Pipeline

We will cover all these topics with sample implementations in Python / TensorFlow / Keras environment.

We will use a Kaggle Dataset in which there are 32 topics and more than 400K total reviews.

At the end of this tutorial, we will be able to generate text using a GPT2 transformer model trained on a Turkish review dataset as below:
